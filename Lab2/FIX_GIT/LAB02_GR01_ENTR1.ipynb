{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 2 - Reglas de Asociación y Patrones Secuenciales\n",
    "\n",
    "## Grupo 1 - Paula Hípola Gómez, José Ignacio Navas Sanz y Belén Ortega Pérez\n",
    "\n",
    "## Entregable 1 - Reglas de Asociación\n",
    "\n",
    "### 1.1 (1 punto) Estudia brevemente el dataset (número de usuarios, número de atributos, categorías más frecuentes para cada atributo/columna, etc.) y realiza el preprocesamiento para generar el dataset necesario para aplicar el algoritmo Apriori a este conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "%reset\n",
    "#Importamos las librerías necesarias\n",
    "#Numpy\n",
    "import numpy as np \n",
    "#Pandas\n",
    "import pandas as pd\n",
    "#Scikit-Learn\n",
    "import sklearn as sk\n",
    "#Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#Importamos la libreria mlxtend\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos los datos y los visualizamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File radio.csv does not exist: 'radio.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-97699d7f137f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Importamos el CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'radio.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#Mostramos los datos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File radio.csv does not exist: 'radio.csv'"
     ]
    }
   ],
   "source": [
    "#Importamos el CSV\n",
    "dataset = pd.read_csv('radio.csv')\n",
    "\n",
    "#Mostramos los datos\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('La dimensión del dataset es:',dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos 289955 instancias y 4 atributos, para cada una de nuestras instancias. Analizando más profundamente el dataset, está compuesto con las interacciones de cada usuario que ha escuchado al menos a un artista. Vamos a ver cuantos valores tenemos actualmente en cuanto a usuarios, artistas y demás atributos y luego procederemos a identificar en el dataset valores duplicados y nulos que dificultan la comprensión de los datos y procederemos a su eliminación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analizando los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Valores desconocidos\n",
    "desconocidos = dataset.loc[dataset['artist'] == '[unknown]']\n",
    "print('Hay ', desconocidos.shape[0], 'valores desconocidos.');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numUsuarios = dataset.groupby('user').count().shape[0]\n",
    "print('Hay',numUsuarios,'usuarios.')\n",
    "numArtistas = dataset.groupby('artist').count().shape[0]\n",
    "print('Hay',numArtistas,'artistas.')\n",
    "numSexos = dataset.groupby('sex').count().shape[0]\n",
    "print('Hay',numSexos,'géneros.')\n",
    "numPaises = dataset.groupby('country').count().shape[0]\n",
    "print('Hay',numPaises,'países.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['user'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función anterior nos muestra que el valor máximo para usuarios es de 19718 pero realmente hay 15000 usuarios. Esto se debe a que en la tabla tenemos las transacciones de aquellos que han escuchado al menos una canción pero se ha guardado su ID de registro. Es decir, cuando un Usuario se registra se le asigna un ID, por ejemplo el 1, pero si ese usuario no ha escuchado ninguna canción, no estará en nuestra base de datos. En cambio si el siguiente usuario con ID 4 escucha una canción si le tendremos en nuestro registro, de ahí que el valor máximo de usuarios sea 19718 pero realmente solo tenemos 15000, que son aquellos que si han escuchado al menos una canción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['artist'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Respecto a los artistas, antes hemos obtenido que había 1004 y efectivamente es lo que obtenemos cuando llamamos a la funcón 'describe'. Observamos que el artista más escuchado es radiohead, podremos analizar a este artista con respecto a los demás si tenemos algún interés especial en él."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['sex'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Respecto al género, esta columna no nos aporta un valor diferencial. Simplemente nos indica si el usuario es mujer u hombre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['country'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Respecto al país puede jugar un valor algo más diferencial que el sexo en nuestro dataset pero consideramos que tampoco aporta un valor excesivamente diferencial puesto que nos vamos a centrar en los artistas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Valores desconocidos\n",
    "dataset.loc[dataset['artist'] == '[unknown]']\n",
    "dataset = dataset.replace('[unknown]',np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminación de nulos\n",
    "dataset = dataset.dropna()\n",
    "print('Tamaño sin nulos',dataset.shape)\n",
    "\n",
    "#Eliminación de duplicados\n",
    "dataset = dataset.drop_duplicates()\n",
    "print('Tamaño sin duplicados',dataset.shape)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos que hacer un nuevo dataset para analizar estos datos con el algoritmo Apriori, para ello vamos a construir un dataset con un listado de ususarios con todos los artistas que ha escuchado..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dagrupado = dataset.groupby(['user','sex','country'])['artist'].agg(lambda x: list(x))\n",
    "dfAgrupado = pd.DataFrame(dataset.groupby(['user','sex','country'],as_index=False)['artist'].agg(lambda x: list(x)))\n",
    "dfAgrupado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pese a tener los datos del país del usuario además de su género, nos damos cuenta de que estos datos carecen de valor y no aportan nada al objetivo que perseguimos, que no es más que la adquisición de los conjuntos de grupos de música más frecuentes para la obtención de reglas de asociación que nos permita el estudio de estos grupos y sus similitudes por el cual se produzcan estos agrupamientos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 (4 puntos) Aprende a manejar la librería mlextend y su implementación del algoritmo Apriori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(dagrupado).transform(dagrupado)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Obtén los itemsets frecuentes para k=1. Para esto es necesario obtener el soporte de los itemset. Crea una función que utilice métodos de la librería mlextend de manera que dado un itemset devuelva su soporte.\n",
    "\n",
    "\n",
    "Necesitamos definir un soporte mínimo. Sabemos que tenemos 14995 instancias, por lo que hemos considerado que aquellos valores que esten alrededor de 250/14995 ≈ 0.0167, serán válido para nosotros a la hora de operar con ello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets = apriori(df, min_support=0.010, use_colnames=True)\n",
    "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "frequent_itemsets1 = frequent_itemsets[frequent_itemsets['length'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets1.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "artistas = [];\n",
    "def soporteItemset(itemset):\n",
    "    #Obtengo una lista de artistas\n",
    "    for x in frequent_itemsets1['itemsets']:\n",
    "        artista = (list(x))\n",
    "        artistas.append(artista)\n",
    "    #Recorro la lista de artistas\n",
    "    for i in range(len(artistas)):\n",
    "        if itemset in artistas[i]:\n",
    "            soporte = frequent_itemsets1.loc[i]['support']\n",
    "            return 'El sporte para ' + str(itemset) + ' es de: ' + str(round(soporte,4))\n",
    "    return 'El artista ' + str(itemset) + ' no está en el itemset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coldplay = soporteItemset('coldplay')\n",
    "print(coldplay)\n",
    "\n",
    "ecdl = soporteItemset('el canto del loco')\n",
    "print(ecdl)\n",
    "\n",
    "beatles = soporteItemset('the beatles')\n",
    "print(beatles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2. Crea una función que devuelva los itemsets frecuentes candidatos y su soporte para k>=2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una función que nos irá calculando itemsets frecuentes candidatos a medida que sea posible para k>=2 y que parará cuando no se generen estos itemsets. Esta función nos sirve además para el siguiente apartado donde se pide más o menos lo mismo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def soporteItemsetKMayorIgual(k):\n",
    "    #Creamos un diccionario. Esto servira para concatenarlo al nombre del itemset generado\n",
    "    valoresValidosK = []\n",
    "    data = dict();\n",
    "    numCreados = 0;\n",
    "    if k < 2:\n",
    "        return 'La k debe ser mayor o igual a 2'\n",
    "    else:\n",
    "        while k != -1:\n",
    "            print('Creando para k = ',k)\n",
    "            frq = frequent_itemsets[frequent_itemsets['length'] == k]\n",
    "            if(frq.shape[0] > 0):\n",
    "                print(frq.shape[0])\n",
    "                valoresValidosK.append(k);\n",
    "                var = 'frq'+str(k);\n",
    "                print(var,'Creado')\n",
    "                print('Aumentamos k\\n')\n",
    "                k += 1;\n",
    "                numCreados += 1;\n",
    "            else:\n",
    "                print('No se puede generar un itemset para k = ',k)\n",
    "                print('Paramos el bucle. No generamos nada para k >= ',k)\n",
    "                k = -1;\n",
    "        for i in range(numCreados):\n",
    "            data['frq'+str(valoresValidosK[i])] = frequent_itemsets[frequent_itemsets['length'] == valoresValidosK[i]]        \n",
    "        print('\\nPara obtener los itemsets creados consulte el diccionario que devuelve la funcion.')\n",
    "        print('Se han generado',numCreados,'datasets')\n",
    "        return data,numCreados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemsets, itemSetsGenerados = soporteItemsetKMayorIgual(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3. Crea una función que repita el paso 1.2.2. hasta que no se generen nuevos itemsets frecuentes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función que hemos generado en el paso 1.2.2 ya nos hace automáticamente esta función. Es decir, hemos creado una función que nos genera automáticamente los itemsets para k>=2 y que para cuando no hay itemsets frecuentes, por tanto no es necesario volver a repetir esta función"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k2 = pd.DataFrame.from_dict(itemsets['frq2'])\n",
    "k2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "k3 = pd.DataFrame.from_dict(itemsets['frq3'])\n",
    "k3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4. Crea una función que muestre todas las reglas posibles con su confianza\n",
    "\n",
    "En esta función determinamos la métrica de confianza, ya que esta evaluará si se tratan de reglas de interés o no para ser tenidas en cuenta. Para posteriormente poder hacer un filtrado de las reglas según la confianza deseada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sacarReglas(itemsets):\n",
    "    reglas = association_rules(itemsets, support_only=False, metric=\"confidence\", min_threshold=0.015)\n",
    "    reglas = reglas[['antecedents', 'consequents', 'antecedent support', 'consequent support', 'support', 'lift', 'confidence']]\n",
    "    return reglas\n",
    "sacarReglas(frequent_itemsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.5. Lista todas las reglas que sean de alta confianza (p.ej. con valor >=0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reglasAltaConfi = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.5)\n",
    "reglasAltaConfi = reglasAltaConfi[['antecedents', 'consequents', 'confidence', 'lift']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reglasAltaConfi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.6. Crea funciones que permitan lo siguiente:\n",
    "<ul>\n",
    "    <li><b>* devolver todas las reglas que contengan un antecesor dado</b></li>\n",
    "    <li><b>* devolver todas las reglas que tengan una confianza mayor (o igual) que un umbral mínimo dado</b></li>\n",
    "    <li><b>* devolver todas las reglas que tengan un lift en una de las tres franjas de estudio (under 1, over 1, =1)</b></li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "Para crear las siguientes funciones se emplea la función where(), que les da valores Nan a todas aquellas instancias que no cumplen la condición dada. Una vez hecho esto se eliminan del índice todos aquellos valores Nan, dejando unicamente aquellas reglas que si cumplan con esta condición."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def reglasAntecesor(antecesor):\n",
    "    antecedents = reglasAltaConfi['antecedents']\n",
    "    index = antecedents.where([bool(antecesor in a) for a in list(antecedents)]).dropna().index\n",
    "    return reglasAltaConfi.loc[index, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reglas con un antecesor dado por parámetro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reglasAntecesor('the killers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reglasConfianza(confianza):\n",
    "    confidence = reglasAltaConfi['confidence']\n",
    "    index = confidence.where(confidence >= confianza).dropna().index\n",
    "    return reglasAltaConfi.loc[index, :] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reglas con la confianza mayor o igual que un valor dado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reglasConfianza(0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reglasLift(lft):\n",
    "    lift = reglasAltaConfi['lift']\n",
    "    if lft == 1: \n",
    "        index = lift.where(lift == lft).dropna().index\n",
    "    elif lft > 1:\n",
    "        index = lift.where(lift > lft).dropna().index\n",
    "    elif lft < 1:\n",
    "        index = lift.where(lift < lft).dropna().index\n",
    "    \n",
    "    return reglasAltaConfi.loc[index, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reglas con Lift dado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reglasLift(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reglasLift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reglasLift(1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al clasificar las reglas según su lift nos damos cuenta de que todas nuestras reglas tienen un lift mayor a 1, esto nos indica que tanto el antecedente como el consecuente aparecen juntos más de lo esperado, dándonos a entender que el antecedente tiene un efecto positivo sobre el consecuente de la regla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.7. Utiliza al menos dos representaciones gráficas para mostrar las reglas obtenidas e interpretar los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al no especificar qué reglas debemos mostrar nos hemos decantado por sacar, de las reglas generales la relación de la confianza con el soporte y ver cómo están relacionadas entre ellas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reglasGenerales = sacarReglas(frequent_itemsets)\n",
    "reglasGenerales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soporte = reglasGenerales['support'].values\n",
    "confianza = reglasGenerales['confidence'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos todas las reglas generales. Ahora pondremos la relación entre soporte y confianza en una gráfica. Donde podemos ver que la mayor variación es en el soporte, ya que la confianza se situa mayoritariamente entre los valores 0.00 y 0.04."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "for i in range (len(soporte)):\n",
    "    soporte[i] = soporte[i] + 0.0025 * (random.randint(1,10) - 5) \n",
    "    confianza[i] = confianza[i] + 0.0025 * (random.randint(1,10) - 5)\n",
    "\n",
    "fig = plt.figure(figsize = (10,8), dpi=360)\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Confianza', fontsize = 10)\n",
    "ax.set_ylabel('Soporte', fontsize = 10)\n",
    "ax.set_title('Relación Soporte - Confianza Reglas Asociación', fontsize = 14)\n",
    "ax.scatter(soporte, confianza, alpha=0.3, marker=\"o\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, como tenemos las reglas generales, buscamos en las reglas generales aquellas que tengan como consecuente un grupo, en nuestro caso vamos a elegir el grupo Queen y veremos como se relacionan antecedentes y consecuentes, es decir, una relación de cómo interactúan los usarios que escuchan Queen. Cuando escuchan a qué artistas, luego escuchan a Queen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queen = reglasGenerales[reglasGenerales['consequents'].astype(str).str.lower().str.contains('queen')]\n",
    "queen.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "antecedentes = []\n",
    "consecuentes = []\n",
    "for t in queen['antecedents']:\n",
    "    texto = str(t);\n",
    "    texto = texto.replace(\"frozenset({\", \"\")\n",
    "    texto = texto.replace(\"})\", \"\")\n",
    "    antecedentes.append(texto)\n",
    "    \n",
    "\n",
    "for t in queen['consequents']:\n",
    "    texto = str(t);\n",
    "    texto = texto.replace(\"frozenset({\", \"\")\n",
    "    texto = texto.replace(\"})\", \"\")\n",
    "    consecuentes.append(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'antecedentes': antecedentes, \n",
    "        'consecuentes': consecuentes} \n",
    "  \n",
    "# Convert the dictionary into DataFrame \n",
    "queen = pd.DataFrame(data) \n",
    "queen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "fig = plt.figure(figsize = (20,10), dpi=360)\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.set_title('Artistas Consecuentes de Queen', fontsize = 20)\n",
    "GA=nx.from_pandas_edgelist(queen, source='antecedentes', target='consecuentes')\n",
    "nx.draw(GA,with_labels=True)\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Podemos observar que Queen está relacionado directamente con grupos como The Killers, The Who o incluso Pink Floyd. Tiene una relación curiosa con Led Zeppelin, mediante la que descubrimos que se une a Queens of the Stone Age descubriendo aquí una nueva relación entre Queen y estos últimos mediante Led Zeppelin. Ocurre lo mismo con The Beatles y una consecuente subconexión con Pink Floyd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
